# -*- mode:org; -*-

#+title:A Guide to Securing Node.js Applications
#+subtitle:Educative {{{version}}} {{{date}}}
#+author:WLH4
#+date:2020-12-10 21:49
#+macro:version Version 0.0.1
#+macro:upload-date (eval (current-time-string))
#+bucket:pinecone-forest.com

{{{version}}} {{{date}}}

#+texinfo:@insertcopying


* Introduction
:PROPERTIES:
:unnumbered: t
:created_by: Ben Edmunds
:level:    intermediate
:lessons:  36
:quizzes:  7
:playgrounds: 20
:code_snippets: 53
:illustrations: 39
:END:
- [[https://github.com/benedmunds/Building-Secure-Node-Apps-Examples][GitHub]]

** Course Overview
This course  is your guide for  securing Node.js applications. You'll  start by
- properly *sanitizing user input and output*, and then
- move on to some *fundamental protocols*, such  as
  - HTTPS and
  - SHA.
- *Passwords* and *encryption* will  be discussed next.
- More specifically, you will learn about different *hashing algorithms* and
- protecting your application from *brute force* attacks.
- Following that,  you'll explore  concepts like
  - authentication,
  - access control, and
  - obfuscation.
- You  will also learn about
  - XSS,
  - CSRF,  and
  - other popular hacks near the end of the course.
- By the end of  this course, you will know how to
  - secure a Node.js application, an in-demand skill to put on your resume!

** Learning Objectives
- Sanitize user input and output.
- Understand and implement HTTPS.
- Create secure hashes using random salts.
- Implement access control with authentication.
- protect yourself against XSS and CSRF.

** Who is this course for
The intended audience and prerequisites for this course.

*** Prerequisites
You  should be  well-versed  in  the fundamentals  of  Node.js before  securing
Node.js applications.

If  you want  a refresher  on  Node.js, check  out our  other Node.js  courses.

Familiarity with Express  is also recommended as the code  snippets assume that
we are working on an [[https://expressjs.com][Express]] application.

*** What this Course Offers
This course serves as  a survival guide for a world full  of security flaws and
hackers

The  course is  for  Node,js developers  looking to  improve  and secure  their
applications. The  course is  full of handy  code snippets,  illustrations, and
real-life stories.

You will begin by
- securing  user input  and learning  how that  input can  open you  up to  SQL
  injection attacks.
- Most importantly, you  will learn to protect yourself and  your users against
  such attacks.
- Then, you will learn about some fundamental protocols and algorithms, such as
  HTTPS,  SSL, and  SHA. Implementing  HTTPS  is a  stepping-stone to  creating
  secure web applications.

- Then,  the course  discusses passwords  and  hashes.

- You  will explore
  - hashing algorithms,
  - password attacks,  and
  - protecting your  application against  such attacks.
- You will then move to authentication  and access control. This will allow you
  to implement user roles and prevent unauthorized access to secure web pages.
- Finally, you will explore XSS and CSRF and how they can be mitigated.

** About the Course
*** Introduction
Several years ago,  I wrote a web  application for a client  in the CodeIgniter
PHP   framework.  At   the  time,   CodeIgniter  didn’t   include  a   built-in
authentication system. I did what any good  developer does and went on the hunt
for a well-made library to supply authentication capabilities. To my chagrin, I
fail to find any concise libraries that  fit my needs. Thus began my journey of
creating  Ion Auth,  a simple  authentication  library for  CodeIgniter, and  a
career-long crusade to secure web applications for myself and other developers.

Now, many  have moved to other  frameworks or languages, but  basic security is
often  overlooked. Let’s  fix  that.  Let’s avoid  the  horror  of leaked  user
passwords, malicious SQL into your database or any other avoidable hacks.

This course is short with handbook-style  references to specific items that can
better secure your  application. You can complete  it in a couple  of hours and
then reference later as needed. And let’s have some fun in the process.

* Sanitize ALL Input

** SQL Injection
Learn about how SQL injection works and its impact.

*** Storytime
Let’s start with a story. Mike is  the system admin for a small private school.
His main responsibility is maintaining  the network and computers. Recently, he
started  automating  various  tasks  around   the  school  by  building  a  web
application for internal use. He has no formal training and minimal programming
experience.  Knowing the  basics  of PHP,  he built  a  pretty stable  customer
relationship  manager  for  the  school   and  even  received  kudos  from  the
superintendent for streamlining operations and saving the school money.

Everything was going well for Mike  until a particular new student started. The
student’s  name is  Little Bobby  Tables. One  day, Jon  from the  admin office
called Mike to ask  why the system was down. After  inspection, Mike found that
the table  containing all the  students’ information was missing  entirely. You
see, Little  Bobby’s full name  is actually “Robert’); DROP  TABLE students;–”.
There aren’t any  backups of the database;  it has been on Mike’s  “to do” list
for a while, but he hadn’t gotten around to it yet. Mike is in big trouble.

*** Real World

#+cindex:SQL injection vulnerability
While it’s unlikely a child’s name will  contain harmful SQL code, this kind of
*SQL injection vulnerability* happens in the real world all the time:

- In 2012, LinkedIn leaked over 6 million users’ data due to an undisclosed SQL
  injection vulnerability.
- In 2012, Yahoo! exposed 450,000 user passwords.
- In 2012, 400,000 passwords were compromised from Nvidia.
- In 2012, 150,000 passwords were compromised from Adobe.
- In 2013, Harmony had roughly 1.5 million user passwords exposed.

For most of these, precise details are undisclosed. We can’t be sure these were
due to SQL injection attacks, but it is likely.

*** How SQL Injection Works
If  you  use  user  input  without modification,  a  malicious  user  can  pass
unexpected data and fundamentally change your SQL queries.

If your code looks something like this:

#+begin_src sql
  UPDATE users
      SET first_name="' + req.body.first_name +  '" WHERE id=1001;
#+end_src

You would expect the generated SQL to be:

#+begin_src sql
  UPDATE users 
      SET first_name="Liz" WHERE id=1001;
#+end_src

But if your malicious user types their first name as:

: Liz", last_name="Lemon"; --

The generated SQL then becomes:

#+begin_src sql
  UPDATE users
      SET first_name="Liz", last_name="Lemon"; --" WHERE id=1001;
#+end_src

Now all of your users are named Liz Lemon, and that’s just not cool.

*** SQL Injection in Action
If preventive  measures are  not taken,  SQL injection  attacks can  cause many
problems. Let’s inject  some SQL code. Hitting  the RUN button will  open a web
application. The edit function is vulnerable. Try to exploit it!

Your app can be found at:
- https://6e5movo1xd239.educative.run/

#+name:sql-injection-package.json
#+begin_src js
  {
    "name": "psql-demo",
    "version": "0.1.0",
    "description": "PSQL demo for Securing Node JS Apps",
    "main": "index.js",
    "scripts": {
      "test": "echo \"Error: no test specified\" && exit 1",
      "start": "nodemon index.js",
      "start:prod": "node index.js",
      "build": "cd client && npm install && npm run build",
      "client": "npm start --prefix client",
      "dev": "concurrently \"npm run start\" \"npm run client\""
    },
    "author": "Educative",
    "license": "MIT",
    "dependencies": {
      "body-parser": "^1.18.3",
      "cors": "^2.8.5",
      "express": "^4.16.4",
      "concurrently": "^5.3.0",
      "nodemon": "^2.0.4",
      "password-generator": "^2.2.0",
      "pg": "^8.3.3",
      "pg-hstore": "^2.3.3",
      "sequelize": "^6.3.5",
      "http-proxy-middleware": "^1.0.5"
    },
    "devDependencies": {
      "prettier": "^1.18.2"
    }
  }
#+end_src

#+name:sql-injection-index.js
#+begin_src js
  const express = require("express");
  const bodyParser = require("body-parser");
  const cors = require("cors");

  const app = express();

  const routeStudents = require("./src/routes/students");

  app.use(bodyParser.urlencoded({ extended: false }));
  app.use(bodyParser.json());
  app.use(cors());

  app.get("/", (req, res) => {
    res.send("Backend for queries app is working!");
  });

  app.use("/api/students", routeStudents, (req, res) => res.sendStatus(401));

  const port = 3000;
  app.listen(port);

  console.log(`listening on ${port}`);

#+end_src

**** client

#+name:sql-injection-client-package.json
#+begin_src js
  {
    "name": "psql-demo",
    "version": "0.1.0",
    "private": true,
    "dependencies": {
      "@material-ui/core": "^4.11.0",
      "@material-ui/icons": "^4.9.1",
      "material-table": "^1.69.1",
      "react": "^16.7.0",
      "react-dom": "^16.7.0",
      "react-scripts": "2.1.2"
    },
    "scripts": {
      "start": "PORT=15000 react-scripts start",
      "build": "react-scripts build",
      "test": "react-scripts test",
      "eject": "react-scripts eject"
    },
    "eslintConfig": {
      "extends": "react-app"
    },
    "browserslist": [
      ">0.2%",
      "not dead",
      "not ie <= 11",
      "not op_mini all"
    ]
  }
#+end_src

***** public
- favicon,ico

#+name:sql-injection-client-public-index.html
#+begin_src html
  <!DOCTYPE html>
  <html lang="en">
    <head>
      <meta charset="utf-8" />
      <link rel="shortcut icon" href="%PUBLIC_URL%/favicon.ico" />
      <meta
	name="viewport"
	content="width=device-width, initial-scale=1, shrink-to-fit=no"
      />
      <meta name="theme-color" content="#000000" />
      <!--
	manifest.json provides metadata used when your web app is added to the
	homescreen on Android. See https://developers.google.com/web/fundamentals/web-app-manifest/
      -->
      <link rel="manifest" href="%PUBLIC_URL%/manifest.json" />
      <!--
	Notice the use of %PUBLIC_URL% in the tags above.
	It will be replaced with the URL of the `public` folder during the build.
	Only files inside the `public` folder can be referenced from the HTML.

	Unlike "/favicon.ico" or "favicon.ico", "%PUBLIC_URL%/favicon.ico" will
	work correctly both with client-side routing and a non-root public URL.
	Learn how to configure a non-root public URL by running `npm run build`.
      -->
      <title>simple-mern</title>
    </head>
    <body>
      <noscript>You need to enable JavaScript to run this app.</noscript>
      <div id="root"></div>
      <!--
	This HTML file is a template.
	If you open it directly in the browser, you will see an empty page.

	You can add webfonts, meta tags, or analytics to this file.
	The build step will place the bundled scripts into the <body> tag.

	To begin the development, run `npm start` or `yarn start`.
	To create a production bundle, use `npm run build` or `yarn build`.
      -->
    </body>
  </html>
#+end_src

#+name:sql-injection-client-public-manifest.json
#+begin_src js
  {
    "short_name": "simple-mern",
    "name": "simple-mern",
    "icons": [
      {
	"src": "favicon.ico",
	"sizes": "64x64 32x32 24x24 16x16",
	"type": "image/x-icon"
      }
    ],
    "start_url": ".",
    "display": "standalone",
    "theme_color": "#000000",
    "background_color": "#ffffff"
  }
#+end_src

***** src

#+name:sql-injection-client-App.css
#+begin_src css
  .App {
    max-width: 600px;
    margin: auto;
    margin-top: 2em;
    padding: 1em;
    background: white;
    border: 1px solid #ddd;
    border-radius: 8px;
    box-shadow: 0 4px 8px 0px rgba(0, 0, 0, 0.1);
  }

  h1 {
    font-weight: normal;
    margin: 0;
    padding-bottom: 8px;
  }

  .tasks {
    list-style: none;
    padding: 0;
  }

  .done {
    text-decoration: line-through;
    opacity: 0.5;
  }

  label {
    vertical-align: top;
  }

  .delete-button {
    padding-top: 4px;
    margin-left: 8px;
    cursor: pointer;
    opacity: 0.3;
    visibility: hidden;
  }

  li:hover .delete-button {
    visibility: visible;
  }

  .delete-button:hover {
    opacity: 0.6;
  }

  table {
    font-family: arial, sans-serif;
    border-collapse: collapse;
    width: 100%;
    padding: 8px;
  }

  td,
  th {
    border: 1px solid #dddddd;
    text-align: center;
    padding: 8px;
  }

  tr:nth-child(even) {
    background-color: #dddddd;
  }
#+end_src

#+name:sql-injectin-client-App.js
#+begin_src js
  import React, { Component } from "react";
  import { TextField } from "@material-ui/core";
  import Button from "@material-ui/core/Button";
  import "./App.css";

  import StudentList from "./components/StudentList";

  class App extends Component {
    state = {
      new_first_name: "",
      new_last_name: "",
      new_id: "",
    };

    handleChange = (event) => {
      this.setState({ [event.target.name]: event.target.value });
    };

    clickAddStudent = (event) => {
      event.preventDefault();

      const first_name = this.state.new_first_name;
      const last_name = this.state.new_last_name;
      const id = this.state.new_id;
      const url = window.location.href.slice(0, -1);

      fetch(url + ":3000/api/students/add", {
	method: "post",
	headers: { "Content-Type": "application/json" },
	body: JSON.stringify({ first_name, last_name, id }),
      }).then(() => {
	this.setState({ newStudentTitle: "" });
	this.refs.studentList.getStudents();
      });
    };

    render() {
      return (
	<React.Fragment>
	  <div className="App">
	    <StudentList ref="studentList" />
	    <form onSubmit={this.clickAddStudent}>
	      <TextField
		variant="outlined"
		margin="normal"
		required
		fullWidth
		id="new_first_name"
		label="First Name"
		name="new_first_name"
		autoComplete="first_name"
		autoFocus
		onChange={this.handleChange}
	      />
	      <TextField
		variant="outlined"
		margin="normal"
		required
		fullWidth
		id="new_last_name"
		label="Last Name"
		name="new_last_name"
		autoComplete="last_name"
		onChange={this.handleChange}
	      />
	      <TextField
		variant="outlined"
		margin="normal"
		required
		fullWidth
		id="new_id"
		label="ID"
		name="new_id"
		autoComplete="id"
		onChange={this.handleChange}
	      />
	      <Button type="submit" fullWidth variant="contained" color="primary">
		Add
	      </Button>
	    </form>
	  </div>
	</React.Fragment>
      );
    }
  }

  export default App;
#+end_src

#+name:sql-injection-client-index.css
#+begin_src css
  body {
    margin: 0;
    padding: 0;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Oxygen",
      "Ubuntu", "Cantarell", "Fira Sans", "Droid Sans", "Helvetica Neue",
      sans-serif;
    -webkit-font-smoothing: antialiased;
    -moz-osx-font-smoothing: grayscale;
    font-size: 16px;
  }

  code {
    font-family: source-code-pro, Menlo, Monaco, Consolas, "Courier New",
      monospace;
  }
#+end_src

#+name:sql-injection-client-index.js
#+begin_src js
  import React from 'react';
  import ReactDOM from 'react-dom';
  import './index.css';
  import App from './App';

  ReactDOM.render(<App />, document.getElementById('root'));
#+end_src

****** components

#+name:sql-injection-client-components-StudentList.js
#+begin_src js
  import React, { Component, forwardRef } from "react";
  import { Container } from "@material-ui/core/";
  import MaterialTable from "material-table";
  import {
    AddBox,
    ArrowUpward,
    Check,
    ChevronLeft,
    ChevronRight,
    Clear,
    DeleteOutline,
    Edit,
    FilterList,
    FirstPage,
    LastPage,
    Print,
    Remove,
    SaveAlt,
    Search,
    ViewColumn,
  } from "@material-ui/icons/";

  export default class StudentList extends Component {
    state = {
      columns: [
	{ title: "First Name", field: "first_name" },
	{ title: "Last Name", field: "last_name" },
	{ title: "ID", field: "id", type: "numeric" },
      ],
      data: [],
      loaded: false,
    };

    componentDidMount() {
      this.getStudents();
    }

    getStudents = () => {
      const url = window.location.href.slice(0, -1);
      fetch(url + ":3000/api/students")
	.then((res) => res.json())
	.then((students) => {
	  if (students.name === "SequelizeDatabaseError") {
	    console.log("Empty!");
	  } else {
	    this.setState({
	      data: [...students],
	    });
	  }
	});
    };

    clickUpdateStudent = (old, student) => {
      const url = window.location.href.slice(0, -1);
      var { first_name, last_name, id } = student;
      fetch(url + `:3000/api/students/update/${old.id}`, {
	method: "post",
	headers: { "Content-Type": "application/json" },
	body: JSON.stringify({ first_name, last_name, id }),
      }).then((res) => res.json());
    };

    clickDeleteStudent = (studentId) => {
      const url = window.location.href.slice(0, -1);
      fetch(url + `:3000/api/students/delete/${studentId}`, {
	method: "delete",
      })
	.then((res) => res.json())
	.catch((err) => console.log(err));
    };

    render() {
      const tableIcons = {
	Add: forwardRef((props, ref) => <AddBox {...props} ref={ref} />),
	Check: forwardRef((props, ref) => <Check {...props} ref={ref} />),
	Clear: forwardRef((props, ref) => <Clear {...props} ref={ref} />),
	Delete: forwardRef((props, ref) => (
	  <DeleteOutline {...props} ref={ref} />
	)),
	Edit: forwardRef((props, ref) => <Edit {...props} ref={ref} />),
	Export: forwardRef((props, ref) => <SaveAlt {...props} ref={ref} />),
	Filter: forwardRef((props, ref) => <FilterList {...props} ref={ref} />),
	FirstPage: forwardRef((props, ref) => <FirstPage {...props} ref={ref} />),
	LastPage: forwardRef((props, ref) => <LastPage {...props} ref={ref} />),
	NextPage: forwardRef((props, ref) => (
	  <ChevronRight {...props} ref={ref} />
	)),
	PreviousPage: forwardRef((props, ref) => (
	  <ChevronLeft {...props} ref={ref} />
	)),
	Print: forwardRef((props, ref) => <Print {...props} ref={ref} />),
	ResetSearch: forwardRef((props, ref) => <Clear {...props} ref={ref} />),
	Search: forwardRef((props, ref) => <Search {...props} ref={ref} />),
	SortArrow: forwardRef((props, ref) => (
	  <ArrowUpward {...props} ref={ref} />
	)),
	ThirdStateCheck: forwardRef((props, ref) => (
	  <Remove {...props} ref={ref} />
	)),
	ViewColumn: forwardRef((props, ref) => (
	  <ViewColumn {...props} ref={ref} />
	)),
      };
      return (
	<Container maxWidth="xl" disableGutters={true}>
	  <React.Fragment>
	    <MaterialTable
	      title="Records table"
	      columns={this.state.columns}
	      data={this.state.data}
	      editable={{
		onRowUpdate: (newData, oldData) =>
		  new Promise((resolve, reject) => {
		    setTimeout(() => {
		      let data = [...this.state.data];
		      const index = data.indexOf(oldData);
		      data[index] = newData;
		      this.clickUpdateStudent(oldData, newData);
		      this.setState({
			data: data,
		      });
		      resolve();
		    }, 1000);
		  }),
		onRowDelete: (oldData) =>
		  new Promise((resolve, reject) => {
		    setTimeout(() => {
		      {
			let data = this.state.data;
			const index = data.indexOf(oldData);
			const del_id = data[index].id;
			this.clickDeleteStudent(del_id);
			this.setState({
			  data: data.filter((student) => student.id !== del_id),
			});
			this.getStudents();
		      }
		      resolve();
		    }, 1000);
		  }),
	      }}
	      icons={tableIcons}
	      options={{
		rowStyle: {
		  backgroundColor: "#FFFFFF",
		},
		headerStyle: {
		  backgroundColor: "#EEE",
		  fontWeight: "bold",
		  fontSize: 16,
		},
	      }}
	    />
	  </React.Fragment>
	</Container>
      );
    }
  }
#+end_src

**** src

#+name:sql-injection-src-db.js
#+begin_src js
  const Sequelize = require("sequelize");

  const sequelize = new Sequelize(
    "postgres://docker:docker@127.0.0.1:5432/postgres"
  );

  sequelize
    .authenticate()
    .then(() => {
      console.log("Connection has been established successfully.");
    })
    .catch((err) => {
      console.error("Unable to connect to the database:", err);
    });

  module.exports = sequelize;
#+end_src

***** models

#+name:sql-injection-src-models-student.js
#+begin_src js
  const sequelize = require("../db");
  const Sequelize = require("sequelize");

  const Student = sequelize.define(
    "student",
    {
      first_name: {
	type: Sequelize.STRING,
	allowNull: false,
      },
      last_name: {
	type: Sequelize.STRING,
	allowNull: false,
      },
      id: {
	type: Sequelize.INTEGER,
	allowNull: false,
	primaryKey: true,
      },
    },
    {
      timestamps: false,
    }
  );

  module.exports = Student;
#+end_src

***** routes

#+name:sql-injection-src-routes-students.js
#+begin_src js
  const express = require("express");
  const Student = require("../models/student");
  const router = express.Router();
  var pg = require("pg");

  const client = new pg.Client({
    user: "docker",
    database: "postgres",
    password: "docker",
    port: 5432,
  });
  client.connect();

  router.get("/", async (req, res) => {
    try {
      const users = await Student.findAll({});
      res.json(users);
    } catch (error) {
      res.status(500).json(error);
    }
  });

  router.post("/add", (req, res) => {
    var whitelist = ["first_name", "last_name", "id"];
    var data = {};

    for (var property in req.body) {
      if (
	req.body.hasOwnProperty(property) &&
	whitelist.indexOf(property) !== -1
      ) {
	data[property] = req.body[property];
      }
    }
    const newStudent = new Student(data);

    newStudent
      .save()
      .then((student) => res.json(student))
      .catch((err) => res.status(500).json(err));
  });

  router.delete("/delete/:id", async (req, res) => {
    const idToDel = req.params.id;
    console.log(req.params.id);
    try {
      const success = await Student.destroy({ where: { id: idToDel } });
    } catch {
      (err) => res.json(500, err);
    }
    const users = await Student.findAll({});
    res.json(users);
  });

  router.post("/update/:id", (req, res) => {
    console.log(
      "UPDATE students SET first_name= '" +
	req.body.first_name +
	"', last_name='" +
	req.body.last_name +
	"', id='" +
	req.body.id +
	"' WHERE id='" +
	req.params.id +
	"';"
    );
    client.query(
      "UPDATE students SET first_name= '" +
	req.body.first_name +
	"', last_name='" +
	req.body.last_name +
	"', id='" +
	req.body.id +
	"' WHERE id='" +
	req.params.id +
	"';",
      (err, result) => {
	console.log("Updated");
	if (err) {
	  console.log(err);
	}
      }
    );
  });

  module.exports = router;
#+end_src

* HTTPS

* Password Encryption and Storage

* Authentication--Access Control--Safe File Handling

* Safe Defaults--Cross-Site Scripting

* Build Tools
:PROPERTIES:
:appendix: t
:custom_id: build-tools
:END:
** Makefile					:dependencies:env_vars:perl:
:PROPERTIES:
:appendix: t
:dependency1: make
:dependency2.0: AWS User account at https://aws.amazon.com
:dependency2.1: AWS cli v2 in PATH https://docs.aws.amazon.com/cli/index.html
:dependency2.2: See how to Install AWS CLI v2 at https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-mac.html
:dependency2.3: aws credentials: access token and secret access token stored in ~/.aws/credentials
:dependency2.4: AWS S3 buckets set up for serving a static web page
:dependency3: GitHub Account with personal access token stored in GITHUB_TOKEN
:dependency4: texinfo @6.7._
:dependency5: Emacs, Org-mode, Babel language 'shell' enabled
:env_var1: SYNC_ORG_TEMPLATE: holds the full path to this Template.org file
:env_var2: GITHUB_TOKEN: holds the GitHub personal access token
:env_var3: EDITOR: must hold a reference to a working emacsclient server
:env_var4: COLORS
:END:

#+pindex:Makefile
#+name:Makefile
#+header: :tangle Makefile
#+begin_src makefile

  ###############################################################################
  ### USER-DEPENDENT VARIABLES
  ### USE ENVIRONMENT VARIABLES WHENEVER POSSIBLE

  # NOTE: All environment variables need to be exported PRIOR to starting the
  # Emacs server as EDITOR in your shell startup files; otherwise, they will not
  # be available to Emacs.
  # When I moved from using Bash to Zsh, I inadvertently changed the order of
  # import, and started the Emacs server before importing, and caused a horrible
  # bug which caused the program to work on one computer but fail on another.

  # The absolute path to this Template file
  TEMPLATE := $(SYNC_ORG_TEMPLATE)


  ### TOOLS & RESOURCES
  # tools is a directory holding tangled scripts, such as cmprpl
  # resources is a directory holding static resources for the project
  # images is a directory holding jpg and png image files
  RESOURCES := resources
  TOOLS	    := $(RESOURCES)/tools
  IMAGES    := $(RESOURCES)/images
  CMPRPL    := $(TOOLS)/cmprpl

  # Use emacsclient as $EDITOR; make sure it is set in a shell startup file and
  # the server has been started.
  EMACS	  := $(EMACS)
  EDITOR  := $(EDITOR)

  # User’s personal GitHub token for authentication to GitHub
  # DO NOT HARD-CODE THIS VALUE
  GITHUB_TOKEN := $(GITHUB_TOKEN)

  # The AWS Command Line Interface (AWS CLI) is an open source tool
  # that enables you to interact with AWS services using commands in
  # your command-line shell.  It must be present on your system.  Run the 'make'
  # command 'install-aws-cli' to install it if you do not have it.  Be sure to
  # run 'aws configure' after installing it.  This will place your AWS
  # credentials into ~/.aws/credentials.
  AWS := aws
  S3  := $(AWS) s3
  CFD := $(AWS) cloudfront

  ### END OF USER-DEPENDENT VARIABLES
  ###############################################################################
  ### MAKE-GENERATED VARIABLES

  ### PROJ AND ORG
  # ORG is the name of this Org file with extension .org
  # PROJ is the project name---the Org file name without extension.

  ### NOTE: there can be only one Org file in the project directory;
  # so far this has not been a problem, but it might be.

  PWD  := $(shell pwd)
  ORG  := $(shell ls *.org)
  PROJ := $(basename $(ORG))

  ### NOTE: S is needed only for the Template file because of the way it is nested
  # one level deep in the Templates GitHub repo, which uses the plural form
  # of Templates, whereas this file uses the singular form, Template.  So when
  # the homepage link is updated, the curl command must be told to use the plural
  # form.	 This is obviously a hack only for my own use and can be removed once
  # I clean up this anomaly.

  ifeq ($(PROJ),$(basename $(notdir $(TEMPLATE))))
  S := s
  endif

  # The AWS S3 bucket to use to store the html source file; it is found at the
  # key #+bucket towards the beginning of the file and should include the appropriate
  # suffix (.com, .net, .org, etc)
  BUCKET       := $(shell $(EDITOR) --eval \
		 '(with-current-buffer (find-file-noselect "$(ORG)") \
		    (save-excursion \
		      (goto-char (point-min)) \
		      (re-search-forward "^\#[+]bucket:\\(.*\\)$$" nil t) \
		      (match-string-no-properties 1)))')
  S3_BUCKET    := s3://$(BUCKET)

  # Buckets set up to serve static web sites from S3 can use either http
  # or https protocols; some  http protocols will automatically redirect
  # to https;  however, some only use  http. I would like  to accomodate
  # both, and  so this code  finds the url's  that are in  my Cloudfront
  # account, which presumably will serve https.  If the url is not here,
  # then this must be set up to serve http instead.
  HTTP_S := $(shell $(CFD) list-distributions | perl -MJSON::PP -e \
	  '$$/=""; \
	   my @urls = (); \
	   my $$json=JSON::PP->new->decode(<STDIN>); \
	   for my $$item ( @{$$json->{"DistributionList"}{"Items"}} ) { \
		  push @urls, @{$$item->{"Aliases"}{"Items"}}; \
	   } \
	  my $$found = grep { /'$(BUCKET)'/ } @urls; \
	  print "http", ($$found ? "s" : "");')

  HTTPS_BUCKET := https://$(BUCKET)

  ### DIR, SRC
  # DIR is the .info name found at '#+texinfo_filename:<DIR>.info' (at
  # the bottom of this file in the export configuration settings)
  # without its extension, used as the INFO filename and the name of the
  # HTML export directory; this code uses the lowercased PROJ name if
  # there is no '#+texinfo_filename'.
  # SRC is HTML directory based upon the DIR name

  #DIR := $(shell $(EDITOR) --eval \
  #	'(with-current-buffer (find-file-noselect "$(ORG)") \
  #		(save-excursion \
  #		(goto-char (point-min)) \
  #		(re-search-forward "^\#[+]\\(?:texinfo_filename\\|TEXINFO_FILENAME\\):\\(.*\\).info$$" nil t) \
  #		(match-string-no-properties 1)))')

  DIR := $(shell sed -E -n "/^\#\+texinfo_filename/s/^.*:(.*)\.info$$/\1/p" $(ORG))
  ifeq ($(DIR),$(EMPTY))
	  DIR := $(shell echo $(PROJ) | tr "[:upper:]" "[:lower:]")
  endif

  SRC := $(DIR)/

  ### VERS: v1.2.34/
  # VERS is the version number of this Org document.
  # When sync is run after the version number has been updated, then VERS
  # picks up the newly-changed value.  VERS used to be staticly imbedded
  # when the Makefile was tangled, but it needs to be dynamic for
  # development.

  # QUERY: should this number be formatted like this, or should it be just the numbers?
  # The reason it includes them is the S3PROJ obtains the name from the S3 bucket, and
  # it includes them.  But it only includes them because I have made it so.  Not a good
  # reason just by itself.  The ending slash is not actually a part of the version, but
  # comes from the way the 'aws2 ls' command returns its values.	So VERS should probably
  # not include the trailing slash, although it doesn’t hurt anything.

  VERS := v$(shell $(EDITOR) --eval \
	  '(with-current-buffer (find-file-noselect "$(ORG)") \
		  (save-excursion \
		    (goto-char (point-min)) \
		    (re-search-forward "^\#[+]\\(?:macro\\|MACRO\\):version Version \\(\\(?:[[:digit:]]+[.]?\\)\\{3\\}\\)") \
		    (match-string-no-properties 1)))')/

  ### AWS
  # PROJ_LIST contains the list of projects currently uploaded to
  # the S3 bucket; each item contains the name of the project and its
  # current version.

  # Created function using elisp instead of the shell.
  # This variable contains an elisp list of strings of the form '("proj1-v1.2.3/" "proj2-v4.5.6/" ...)'
  # However, when it prints to the shell, the quotes are lost.
  # Need to make sure elisp's variable 'exec-path contains the proper $PATH instead of adding to 'exec-path.

  PROJ_LIST := $(shell $(EDITOR) --eval \
	  "(progn \
		  (require (quote seq)) (add-to-list (quote exec-path) (quote \"/usr/local/bin\")) \
		  (seq-map (lambda (s) (replace-regexp-in-string \"^\s+PRE \" \"\" s)) \
			  (seq-filter (lambda (s) (string-match-p (regexp-quote \" PRE \") s)) \
			  (process-lines \"$(AWS)\" \"s3\" \"ls\" \"$(S3_BUCKET)\"))))")

  ### S3PROJ
  # The name of the current project as obtained from S3: 'proj-v1.2.34/'
  # If there is no current project in the S3 bucket, then assign a value equal to
  # the Org project and version instead.  It is set to the project if found, and
  # NO if not found, then updated in the ifeq block below.
  S3PROJ := $(shell $(EDITOR) --eval \
		  '(let ((proj (seq-find (lambda (s) (string-match-p "$(DIR)" s)) (quote $(PROJ_LIST))))) \
		     (or proj (quote NO)))')

  ### PROJINS3
  # is used by make sync; this allows the index.html file to be generated the first
  # time the project is synced.  It is set to NO if this project is not currently in an
  # S3 bucket, and it is set to YES if it is.
  PROJINS3 :=

  ### S3VERS
  # The version of this project currently installed in the S3 bucket: 'v1.2.34/'
  # If there is no current version in the S3 bucket, then assign the version from
  # this Org file instead.
  S3VERS   :=

  # Update S3PROJ, S3VERS, and PROJINS3
  ifeq ($(S3PROJ), NO)
	  S3PROJ := $(DIR)-$(VERS)
	  S3VERS := $(VERS)
	  PROJINS3 := NO
  else
	  S3VERS := $(subst $(DIR)-,,$(S3PROJ))
	  PROJINS3 := YES
  endif

  ### GITHUB
  # USER is the current user's GitHub login name.

  # The user name used to be statically embedded into the Makefile
  # during tangle, but in an effort to make the Makefile dynamically
  # indepedent, dynamic code has replaced the static code.  The code
  # that placed the static name in the Makefile was a 'node' script that
  # ran in a separate Org process during tangle.	An unfortunate fact of
  # 'make' is that 'make' strips the quote marks from the string
  # obtained from the 'curl' command when the 'make shell' command
  # returns the string.	 This makes the string malformed JSON and
  # unparsable by most JSON parsers, including 'node’.	However,
  # 'perl'’s core module JSON::PP (but not JSON::XS) has facilities to
  # parse very malformed JSON strings.	Therefore, this dynamic code
  # uses 'perl' and the core module JSON::PP to parse the 'curl' string
  # into a 'perl' JSON object which can return the login name.	This
  # code should work with any version of 'perl' without having to
  # install any modules.

  USER	:= $(shell \
	    curl -sH "Authorization: token $(GITHUB_TOKEN)" https://api.github.com/user \
	    | \
	    perl -MJSON::PP -e \
		'$$/ = ""; \
		 my $$json = JSON::PP->new->loose->allow_barekey->decode(<STDIN>); \
		 print $$json->{login};' \
	    )
  SAVE		:= resources

  ### TEXINFO
  TEXI		:= $(PROJ).texi
  INFO		:= $(DIR).info
  INFOTN	:= $(shell $(EDITOR) --eval "(file-truename \"$(INFO)\")")
  PDF		:= $(PROJ).pdf
  INDEX		:= index.html
  HTML		:= $(DIR)/$(INDEX)
  DIR_OLD	:= $(DIR)-old

  ### AWS S3
  DST_OLD	:= $(S3_BUCKET)/$(S3PROJ)
  DST_NEW	:= $(S3_BUCKET)/$(DIR)-$(VERS)
  EXCL_INCL	:= --exclude "*" --include "*.html"
  INCL_IMAGES	:= --exclude "*" --include "*.jpg" --include "*.png"
  GRANTS	:= --grants read=uri=http://acs.amazonaws.com/groups/global/AllUsers
  S3SYNC	:= $(S3) sync --delete $(EXCL_INCL) $(SRC) $(DST_OLD) $(GRANTS)
  S3MOVE	:= $(S3) mv --recursive $(DST_OLD) $(DST_NEW) $(GRANTS)
  S3COPY	:= $(S3) cp $(INDEX) $(S3_BUCKET) $(GRANTS)
  S3REMOVE	:= $(S3) rm $(S3_BUCKET)/$(S3PROJ) --recursive
  S3IMAGESYNC	:= $(S3) sync $(INCL_IMAGES) $(IMAGES) $(S3_BUCKET)/$(IMAGES) $(GRANTS)

  ###############################################################################

  default: check texi info html pdf

  PHONY: default all check values boot \
	    texi info html pdf \
	    open-org open-texi open-html open-pdf \
	    clean dist-clean wiped-clean \
	    help sync update delete-proj \
	    install-aws-cli \
	    index-html upload-index-html

  values: check
	    @printf "$${BLUE}Values...$${CLEAR}\n"
	    @echo TEMPLATE:	$(TEMPLATE)
	    @echo EDITOR:	$(EDITOR)
	    @echo USER:		$(USER)
	    @echo PWD:		$(PWD)
	    @echo ORG:		$(ORG)
	    @echo TEXI:		$(TEXI)
	    @echo INFO:		$(INFO)
	    @ECHO INFOTN:	$(INFOTN)
	    @echo BUCKET:	$(BUCKET)
	    @echo PROJ:		$(PROJ) $S
	    @echo S3_BUCKET:	$(S3_BUCKET)
	    @echo HTTP_S:	$(HTTP_S)
	    @echo HTTPS_BUCKET:	$(HTTPS_BUCKET)
	    @echo VERS:		$(VERS)
	    @echo S3PROJ:	$(S3PROJ)
	    @echo S3VERS:	$(S3VERS)
	    @echo DIR:		$(DIR)
	    @echo DIR_OLD:	$(DIR_OLD)
	    @echo SRC:		$(SRC)
	    @echo DST_OLD:	$(DST_OLD)
	    @echo DST_NEW:	$(DST_NEW)
	    @echo PROJ_LIST:	"$(PROJ_LIST)"
	    @echo PROJINS3:	$(PROJINS3)

  check:
	    @printf "$${BLUE}Checking dependencies...$${CLEAR}\n"

	    @[[ -z $(BUCKET) ]] && \
	       { printf "$${RED}$(BUCKET) $${CYAN}must be set.$${CLEAR}\n"; exit 1; } || \
	       printf "$${CYAN}BUCKET: $${GREEN}$(BUCKET)$${CLEAR}\n";

	    @[[ -z $${GITHUB_TOKEN} ]] && \
	       { printf "$${RED}GITHUB_TOKEN $${CYAN}must be set.$${CLEAR}\n"; exit 1; } || \
	       printf "$${CYAN}GITHUB_TOKEN: $${GREEN}SET$${CLEAR}\n";

	    @[[ (-d ~/.aws) && (-f ~/.aws/credentials) && (-f ~/.aws/config) ]] && \
	       printf "$${CYAN}AWS credentials and config: $${GREEN}SET$${CLEAR}\n" || \
	       { printf "$${RED}~/.aws 'credentials' and 'config' must be set.$${CLEAR}\n"; exit 1; }

	    @[[ "$(shell $(EDITOR) --eval '(member (quote texinfo) org-export-backends)')" = "(texinfo)" ]] && \
		  printf "$${CYAN}Texinfo backend: $${GREEN}INSTALLED.$${CLEAR}\n" || \
		  { printf "$${YELLOW}Texinfo backend:$${CLEAR} $${RED}NOT INSTALLED; it must be installed.$${CLEAR}\n"; exit 1; }

	    @[[ $(shell $(EDITOR) --eval '(symbol-value org-confirm-babel-evaluate)') == "t" ]] && \
		  { printf "$${YELLOW}org-confirm-babel-evaluate:$${CLEAR} $${RED}T; set to NIL.$${CLEAR}\n"; exit 1; } || \
		  printf "$${CYAN}org-confirm-babel-evaluate: $${GREEN}OFF.$${CLEAR}\n\n"

  open-org: $(ORG)
	    @$(EDITOR) -n $(ORG)
  $(ORG):
	    @echo 'THERE IS NO $(ORG) FILE!!!'
	    exit 1

  texi: $(TEXI)
  $(TEXI): $(ORG)
	   @echo Making TEXI...
	   @$(EDITOR) -u --eval \
		  "(with-current-buffer (find-file-noselect \"$(ORG)\" t) \
			  (save-excursion \
			  (org-texinfo-export-to-texinfo)))"
	   @echo Done making TEXI.
  open-texi: texi
	   @$(EDITOR) -n $(TEXI)

  info: $(INFO)
  $(INFO): $(TEXI)
	   @echo Making INFO...
	   @makeinfo -o $(INFO) $(TEXI)
	   @$(EDITOR) -u -eval \
		  "(when (get-buffer \"$(INFO)\") \
			  (with-current-buffer (get-buffer \"$(INFO)\") \
				  (revert-buffer t t t)))"
	   @echo Done making INFO.

  open-info: info
	   @$(EDITOR) -u -eval \
		  "(if (get-buffer \"*info*\") \
			  (with-current-buffer (get-buffer \"*info*\") \
				(when (not (string= \"(symbol-value (quote Info-current-file))\" \"$(INFOTN)\")) \
					(info \"$(INFOTN)\")) \
				(revert-buffer t t t)) \
		      (info \"$(INFOTN)\"))"

  html: $(HTML)
  $(HTML): $(TEXI)
	   @echo Making HTML INFO..
	   @makeinfo --html -o $(DIR) $(TEXI)
	   @echo Done making HTML.
	   $(CMPRPL) $(DIR) $(DIR_OLD)
  open-html: html
	   @open $(HTML)

  # If pdftexi2dvi produces an error, it may still produce a viable PDF;
  # therefore, use --tidy.  If it produces an error, try to link the PDF;
  # if it does not produce an error, the PDF will be added to the top dir
  # and there will be no attempt to link.
  pdf:	$(PDF)
  $(PDF): $(TEXI)
	  @echo Making PDF INFO...
	  @-pdftexi2dvi --quiet --build=tidy $(TEXI) || ln -s $(PROJ).t2d/pdf/build/$(PDF) $(PDF)
	  @echo Done making PDF.
  open-pdf:pdf
	   @open $(PDF)

  sync:   $(HTML)
	  @echo Syncing version $(VERS) onto $(S3VERS)...
	  $(S3SYNC)
	  $(S3IMAGESYNC)
	  @echo Done syncing.
	  [[ $(VERS) != $(S3VERS) ]] && { echo Moving...; $(S3MOVE); echo Done moving.;  make homepage; } || :
	  [[ $(PROJINS3) = "NO" ]] && make homepage || :

  # This is a target-specific variable for updating the “description”
  # key on the GitHub repo page with the current version number.  It
  # first makes a curl call to the GitHub project repo, finds the
  # “description” line, pulls out the description only (leaving the old
  # version) and then prints the value with the current version number.
  # This value is used by the “homepage:” target in the PATCH call.
  # This method is arguably harder to code but faster to run than using
  # Perl with the JSON::PP module.

  homepage: description = $(shell \
	  curl -s \
		  -H "Authorization: token $(GITHUB_TOKEN)" \
		  https://api.github.com/repos/$(USER)/$(PROJ)$S | \
		  (perl -ne 'if (/^\s*\"description\":\s*\"(.*): v(?:(?:[[:digit:]]+[.]?){3})/) {print $$1}'))

  ### NOTE the use of the S variable at the end of PROJ; this is to handle
  # the singular case of the GitHub repo using the plural form, Templates
  # whereas the the Template.org file uses the singular form.
  homepage: $(ORG) upload-index-html
	    @echo Updating homepage...
	    @echo DESCRIPTION: $(description)
	    @echo VERS: $(VERS)
	    @curl -i \
		  -H "Authorization: token $(GITHUB_TOKEN)" \
		  -H "Content-Type: application/json" \
		  -X PATCH \
		  -d "{\"homepage\":\"$(HTTPS_BUCKET)/$(DIR)-$(VERS)\",\
		       \"description\":\"$(description): $(VERS)\"}" \
		  https://api.github.com/repos/$(USER)/$(PROJ)$S
	    @echo Done updating homepage.

  delete-proj:
	  @echo Deleting project $(PROJ)...
	  @curl -i \
		  -H "Authorization: token $(GITHUB_TOKEN)" \
		  -H "Accept: application/vnd.github.v3+json" \
		  -X DELETE \
		  https://api.github.com/repos/$(USER)/$(PROJ)$S
	  @$(S3REMOVE)
	  @make dist-clean
	  @make upload-index-html
	  @$(EDITOR) -u --eval "(kill-buffer \"$(ORG)\")"
	  @rm -rf "../$(PROJ)"
	  @echo Done deleting project.

  index-html: $(INDEX)
  $(INDEX): $(ORG)
	  @echo making index.html...
	  $(EDITOR) --eval \
	  "(with-current-buffer (find-file-noselect \"$(ORG)\") \
		  (save-excursion \
		    (org-link-search \"#project-index-title\") \
		    (org-export-to-file (quote html) \"index.html\" nil t)))"
	  @echo Done making index.html.

  upload-index-html: $(INDEX)
	   @echo Uploading index.html...
	   $(S3COPY)
	   @echo Done uploading index.html

  install-aws-cli:
	    curl "https://awscli.amazonaws.com/AWSCLIV2.pkg" -o "AWSCLIV2.pkg" && \
	    sudo installer -pkg AWSCLIV2.pkg -target / && \
	    which aws && aws --version
	    rm -rf AWSCLIV2.pkg

  clean:
	  @echo Cleaning...
	    -@rm *~ 2>/dev/null
	    -@for file in *.??*; \
	    do \
		    ext=$${file#$(PROJ).}; \
		    [[ ! $${ext} =~ org|texi|info|pdf|html ]] && rm -rv $${file}; \
	    done

  dist-clean: clean
	  @echo Dist Cleaning...
	    @${EDITOR} -u --eval \
	      "(kill-buffer \"$(ORG)\")"
	    -@rm -rf *.{texi*,info*,html*,pdf*} $(DIR) $(TOOLS)
	    -@for dir in *; \
		do \
		    [ -d $$dir -a $$dir != "$(DIR_OLD)" -a $$dir != $(SAVE) ] && \
		    rm -vr $$dir; \
		done

  wipe-clean: dist-clean
	  @echo Wipe Clean...
	    -@rm -rf Makefile Readme.md $(DIR_OLD)
	    @git checkout Makefile README.md

  git-ready: dist-clean
	    git checkout Makefile
	    git checkout README.md
	    git status

  help:
	    @echo '"make boot" tangles all of the files in Template'
	    @echo '"make default" makes the .texi file, the .info file, \
	    the html files, and the .pdf file.'
	    @echo

	    @echo '"make check" checks for prerequistes'
	    @echo '"make values" runs check and prints variable values'
	    @echo

	    @echo '"make texi" makes the .texi file'
	    @echo '"make info" makes the .info file'
	    @echo '"make html" makes the html distribution in a subdirectory'
	    @echo '"make pdf" makes the .pdf file'
	    @echo

	    @echo '"make open-org" opens the ORG program using emacsclient for editing'
	    @echo '"make open-texi" opens the .texi file using emacsclient for review'
	    @echo '"make open-html" opens the distribution index.html file \
	    in the default web browser'
	    @echo '"make open-pdf" opens the .pdf file'
	    @echo

	    @echo '"make sync" syncs the html files in the AWS S3 bucket BUCKET; \
	    you must have your AWS S3 bucket name in the env var AWS_S3_BUCKET; \
	    You must have your AWS credentials installed in ~/.aws/credentials'
	    @echo

	    @echo '"make install-aws-cli" installs the "aws cli v2" command-line tools'
	    @echo 'You also need to run "aws configure" and supply your Access Key and Secret Access Key'
	    @echo

	    @echo '"make clean" removes the .texi, .info, and backup files ("*~")'
	    @echo '"make dist-clean" cleans, removes the html distribution, \
	    and removes the build directory'
	    @echo '"make wipe-clean" wipes clean the directory, including old directories'
	    @echo

	    @echo '"make delete-proj" deletes the project from the file system, GitHub and AWS'

#+end_src

*** TODO Next
1. The CloudFront configuration needs to be updated recognize the new version
   directory that is created as part of the ~sync~ operation.

2. Update the GitHub HOME website link for each new sync operation.

3. Store on GitHub a version of each other format upon a sync operation (i.e.,
   the INFO and PDF versions)

** Compare Replace

#+begin_comment
The following source code tangles all files during an export operation. This is
to  make  sure  the  ~cmprpl~  source code  exists  in  the  ~resources/tools/~
directory before running  the Makefile target =html=. It also  makes sure there
is a Makefile on an initial export. The following code is not exported.
#+end_comment

#+name:tangle-org-file
#+header: :exports results :eval yes :results silent
#+begin_src emacs-lisp
(org-babel-tangle-file (buffer-file-name))
#+end_src

The  AWS ~sync~  command  relies  upon time  stamps  to  determine whether  two
programs are identical or not, as  well as content.  If two otherwise identical
files have  different time stamps,  ~sync~ will  assume they are  different and
will  process the  newer.   However, the  ~texinfo~  ~makeinfo --html~  command
produces all  new files even  if some files  (or most files)  remain unchanged.
This  means that  all files  will be  uploaded to  the AWS  S3 bucket  on every
iteration, even though the majority of the files are actually unchanged.

The ~cmprpl~  source code attempts to  resolve the issue of  identical exported
code having different  time stamps, thus defeating the benefit  provided by the
~aws2 s3 sync~ command uploading only changed files.

This program makes sure that a generated HTML directory exists: =$DIR_NEW=.  If
it doesn’t, then it is in an improper state and the program stops with an error
message.

The  program then  checks  if  an old  directory  exists,  =$DIR_OLD=.  If  one
doesn’t,  then one  is  created by  copying the  current  new directory.   This
provides a baseline  for comparisons going forward.  The program  exits at that
point. It is very important that  the =$DIR_OLD= directory not be deleted going
forward.

Given  that =$DIR_OLD=  exists, the  program then  loops through  all files  in
=$DIR_NEW= and  compares them  to the  files in =$DIR_OLD=.   If the  files are
identical, the =$DIR_OLD= file replaces the =$DIR_NEW= file while retaining the
old time stamp (using the ~-p~ option of ~cp~. If a file is different, then the
=$DIR_NEW= file  replaces the =$DIR_OLD=  file, thus giving it  updated content
and  an updated  time stamp.   If the  file does  not exist  in the  =$DIR_OLD=
directory, then it is added.

The  program then  loops through  all of  the files  in the  old directory  and
deletes  any that  do not  exist in  the new  directory.  Now  both directories
should be in sync.

#+caption:Compare Replace program
#+name:cmprpl
#+header: :mkdirp t
#+header: :shebang "#!/usr/bin/env bash"
#+begin_src sh :tangle resources/tools/cmprpl
  [[ $# -eq 2 ]] || { echo "ERROR: Incorrect command line arguments"; exit 1; }
  DIR_NEW=$1
  DIR_OLD=$2

  [[ -d $DIR_NEW ]] || { echo "ERROR: $DIR_NEW does not exist"; exit 1; }
  [[ -d $DIR_OLD ]] || { echo "CREATING: $DIR_OLD does not exist"; cp -a $DIR_NEW $DIR_OLD; exit 0; }

  for newfile in $DIR_NEW/*
  do
      oldfile=$DIR_OLD/$(basename $newfile)
      if [[ -e $oldfile ]]
      then
	 if cmp -s $newfile $oldfile
	 then
	     printf "${GREEN}copying OLD to NEW${CLEAR}: "
	     cp -vp $oldfile $newfile
	 else
	     printf "${PURPLE}copying NEW to OLD${CLEAR}: "
	     cp -vp $newfile $oldfile
	 fi
      else
	  printf "${BLUE}creating NEW in OLD${CLEAR}: "
	  cp -vp $newfile $oldfile
      fi
  done

  for oldfile in $DIR_OLD/*
  do
      newfile=$DIR_NEW/$(basename $oldfile)
      if [[ ! -e $newfile ]]
      then
	  printf "${RED}removing OLD${CLEAR}: "
	  rm -v $oldfile
      fi
  done
#+end_src


** Update Utility Commands
*** Get Parsed Org Tree
This function looks for an Org file in the present working directory, and if it
finds one returns  a parsed tree using  ~org-element-parse-buffer~.  It returns
=nil= if there is no Org file or if the found file is not in ~org-mode~.

#+name:get-parsed-org-tree
#+header: :results silent
#+begin_src emacs-lisp
(defun get-parsed-org-tree (&optional org-dir)
  "This function takes an optional directory name, changes to
that directory if given, otherwise uses the pwd, and finds an Org
file and returns its parsed tree, or nil if none found."
  (when org-dir
      (cd (file-name-as-directory org-dir)))
  (let ((buf (car-safe (find-file-noselect "*.org" nil nil t))))
    (if buf
	(with-current-buffer buf (org-element-parse-buffer))
      nil)))
#+end_src

*** Check for CID
This code  checks whether an  Org file contains  a =custom_id= of  a particular
value.  It accepts  a ~cid-value~ and an optional directory.   If the directory
is not given, then it defaults to the current directory.  If throws an error if
the directory does not exist.  It returns =nil= if the given directory does not
contain an Org file.   It returns =t= if the Org file  contains a node property
of   =custom_id=  and   value  ~cid-value~,   or   =nil=  if   not.   It   uses
~get-parsed-org-tree~.

#+name:org-tree-cid-p
#+header: :results silent
#+begin_src emacs-lisp
(defun org-tree-cid-p (cid-value &optional org-dir)
  "Check whether an org file contains a custom_id of CID"
  (let ((tree (get-parsed-org-tree org-dir)))
    (car (org-element-map tree 'property-drawer
	   (lambda (pd) (org-element-map (org-element-contents pd) 'node-property
			  (lambda (np)
			    (and
			     (string= "custom_id" (org-element-property :key np))
			     (string= cid-value (org-element-property :value np))))))
	   nil t))))
#+end_src

#+name:run-org-tree-cid-p
#+header: :var cid="build-tools"
#+header: :var dir="/usr/local/dev/programming/MasteringEmacs"
#+header: :var gpot=get-parsed-org-tree()
#+header: :var otcp=org-tree-cid-p()
#+header: :results value
#+header: :eval never-export
#+begin_src emacs-lisp
(org-tree-cid-p cid dir)
#+end_src

#+call: run-org-tree-cid-p(dir="/usr/local/dev/programming/MasteringEmacs")

** Bucket Index HTML
The bucket should contain a master ~index.html~  file that links to each of the
individual project  ~index.html~ files.  The  master ~index.html~ file  will be
placed at the root of  the bucket, ~https://<bucket-name>.com/~, and the bucket
must be set up to serve this ~index.html~ when the user hits the root.

*** Get Bucket Name
 This  code searches  for  the keyword-value  pair =bucket:<BUCKET-NAME>=  that
 should be  located towards the  beginning of the  file, and returns  the value
 =BUCKET-NAME= or nil if not found.

#+name: get-bucket-name
#+header: :results value
#+begin_src emacs-lisp
   (save-excursion
     (goto-char (point-min))
     (re-search-forward "^#\\+bucket:\\s*?\\(.*\\)$" nil t)
     (match-string-no-properties 1))
#+end_src

For some reason, ~get-bucket-name~ does not  work when called from the headline
[[#project-index-links][=Links for  bucket=]] below  when creating  =index.html=, even  if it  returns as
~(prin1 ...)~ and is  set up to ~:return output~; the  call receives =nil=. The
following code from ~bucket-name~, however, works. I don't know why.

#+name: bucket-name
#+header: :results output
#+header: :var bucket-name=get-bucket-name()
#+begin_src emacs-lisp
(prin1 bucket-name)
#+end_src

*** Bucket HTTPS URL
This  code calls  ~get-bucket-name~ and  returns the  value returned  as a  URL
string or nil.

#+name: bucket-https-url
#+header: :results value
#+header: :var b=get-bucket-name()
#+begin_src emacs-lisp
(concat "https://" b)
#+end_src

*** S3 Bucket URL
This code calls ~get-bucket-name~ and returns the AWS S3 bucket url.

#+name: s3-bucket-url
#+header: :results value
#+header: :var b=get-bucket-name()
#+begin_src emacs-lisp
(concat "s3://" b)
#+end_src

*** Bucket Projects List
This code uses the ~s3-bucket-url~ result to obtain the list of projects in the
bucket.  It does  this by calling the  AWS S3 high-level command  ~ls~ and then
removing the  =PRE= string in  each result.  The result  that is returned  is a
single  string that  can be  separated into  individual links  by breaking  the
string on spaces.

#+name: bucket-projects-list
#+header: :results output
#+header: :var bucket=s3-bucket-url()
#+begin_src sh
/usr/local/bin/aws s3 ls ${bucket} | sed -ne 's/^.*PRE //p'
#+end_src

*** Bucket Project Links
This code  uses the result  from ~bucket-projects-list~ to create  an unordered
list of  links written to  bucket projects, written  in Org-mode syntax.  It is
executed by a =#+call:= in [[*Bucket Index][*Bucket  Index]] during an HTML export of that subtree
to a file called =index.html=.

#+name: bucket-project-links
#+header: :var b-url=bucket-https-url()
#+header: :var projects=bucket-projects-list()
#+header: :results output raw
#+begin_src emacs-lisp
(seq-do (lambda (u) (princ (format "- [[%s/%sindex.html][~%s~]]
" b-url u u))) (split-string projects))
#+end_src

*** Bucket Index
    :PROPERTIES:
    :custom_id: project-index-title
    :export_file_name: index.html
    :export_subtitle: {{{version}}} created {{{upload-date}}}
    :END:
#+html_doctype: html5
#+options: toc:nil html5-fancy:t

#+html: <hr>

**** Links for bucket call_bucket-name()
     :PROPERTIES:
     :unnumbered: t
     :custom_id: project-index-links
     :END:

#+call: bucket-project-links()
** Project Readme
This adds the README.md template to a project. It should be customized uniquely
for the project.

#+name:project-readme
#+header: :tangle README.md
#+begin_src markdown
# TITLE
## Subtitle
## Author
## Date
## Version
# ABSTRACT
This is the Org Template file.	It is the parent of all other Org Info blogs,
and provides the source code for processing them in various different ways.
# INTRODUCTION
# CHAPTER
## Section
### Subsection
#+end_src

** Boot Template
:PROPERTIES:
:dependency1: EMACS:=:/Applications/MacPorts/Emacs.app/Contents/MacOS/Emacs or similar
:dependency2: EDITOR:=:emacsclient
:dependency3: =SYNC_ORG_TEMPLATE= defined as $DEV/Templates/Org/Template.org
:END:
Although running the command ~org-babel-tangle~ (=C-c C-v t=) from within Emacs
will install  everything, it would  be nice to have  a simple Makefile  that is
downloaded with this  file that could be  invoked to do the  same thing without
starting Emacs and Org-mode and keying in the ~org-babel-tangle~ command.  This
little Makefile should be stored on  GitHub along with the ~Template.org~ file.
When  the source  is extracted  to a  directory, then  running this  Makefile's
default rule  as simply ~make~  will extract the ~preprocess.el~  script, which
updates  =DEV= and  then  extracts the  full Makefile.   Because  this file  is
tangled along with the full Makefile, it simply gets tacked onto the end of the
big Makefile as an additional rule.   Now, running ~make~ runs the default rule
from the  main Makefile, which is  to extract everything, then  export to TEXI,
INFO, HTML, and PDF forms.

It is assumed that an Emacs server is running, and that the $EDITOR environment
variable is set to use ~emacsclient~.

#+name:boot-template
#+header: :tangle Makefile
#+begin_src makefile
  boot:
	  $(EDITOR) -u --eval \
		  "(with-current-buffer (car (find-file-noselect \"./*.org\" nil nil t)) \
			  (goto-char (point-min)) \
			  (re-search-forward \"^#[+]name:preprocess.el$$\") \
			  (org-babel-tangle (quote (4))) \
			  (save-buffer) \
			  (kill-buffer))" \
	  --eval \
		  "(let ((rsrcdir \"resources\") \
			 (subdirs (list \"tools\" \"images\"))) \
		     (mkdir rsrcdir t) \
		     (dolist (subdir subdirs) (mkdir (concat rsrcdir \"/\" subdir) t)))"
	  ./resources/tools/preprocess.el
#+end_src

** Preprocess Env Vars
The environment variable DEV can be  in different locations and will be spelled
differently based  on how the  local machine is set  up.  For instance,  on one
system,  it will  be at  ~$HOME/Dev~  while in  another  system it  will be  at
~/usr/local/dev~.  However, the =:tangle= keyword  does not expand variables in
the form ~${DEV}~,  but rather requires absolute  paths, like ~/usr/local/dev~.
Therefore, this program works like a preprocessor for environment variables set
up  as part  of  =:tangle= lines,  changing them  to  their system  environment
variable values prior to tangling.  It lives in the ~resources/tools~ directory.

#+name:preprocess.el
#+header: :mkdirp t
#+header: :tangle resources/tools/preprocess.el
#+header: :shebang "#!/opt/local/bin/emacs -Q --script"
#+begin_src emacs-lisp
  (with-current-buffer (car (find-file-noselect "./*.org" nil nil t))
    (save-excursion
    (goto-char (point-min))
    (let ((re-search-str "\\(?::tangle\\|load-file \\(?:[\\]*\\)?[\"]\\)\s*\\(.*?/[dD]ev\\)/")
          (dev (getenv "DEV")))
      (while
              (re-search-forward re-search-str nil t)
              (replace-match dev t nil nil 1)))
    (save-buffer)
    (require 'org)
    (org-babel-tangle)))
#+end_src

** Samples
#+begin_comment
(cd "~/Dev/Emacs/MasteringEmacs/")
"/Users/pine/Dev/Emacs/MasteringEmacs/"

(defun add-bucket (org bucket)
  "Add a bucket keyword BUCKET to the org file ORG."
  (interactive "fFile: \nsBUCKET: ")
  (with-current-buffer (find-file-noselect org)
    (let* ((tree (org-element-parse-buffer))
	   (ins (car (org-element-map tree (quote section)
		 (lambda (s)
		   (org-element-map s (quote keyword)
		     (lambda (kw) (when (equal "MACRO" (org-element-property :key kw)) (1- (org-element-property :end kw))))
		     nil nil :keyword))
		 nil t nil nil))))
      (goto-char ins)
      (insert (format "#+bucket:%s\n" bucket))
      ())))

(add-bucket "MasteringEmacs.org" "pinecone-forest")
nil

(defun hl-region (raw-hl)
  "Obtain the begin and end positions for a headline."
  (with-current-buffer (find-file-noselect (getenv "SYNC_ORG_TEMPLATE"))
    (let* ((tree (get-parsed-tree))
	   (hl (car-safe (org-element-map tree 'headline
			   (lambda (hl) (when
					    (string= raw-hl
						     (org-element-property :raw-value hl))
					  (org-element-context)))
			   nil nil t))))
      (cons
       (org-element-property :begin hl)
       (org-element-property :end hl))
      )))

(hl-region "Build Tools")

(4888 . 29646)

(defun get-hl-with-prop (org-dir hl-prop)
  "Given a directory containing an Org template file and a custom_id property name, return the headline containing that custom_id, or nil if none."
  (progn
    (cd org-dir)
    (let ((org-buf (car-safe (find-file-noselect "*.org" nil nil t))))
      (if org-buf
	  (with-current-buffer org-buf
	    (let ((tree (org-element-parse-buffer)))
	      (org-element-map tree 'headline
		(lambda (hl)
		  (let ((cid (org-element-property :CUSTOM_ID hl)))
		    (when (string= hl-prop cid)
		      (and
		       (message (format "Found the headline %s containing property %s." (org-element-property :raw-value hl) hl-prop))
		       hl))))
		nil t)))
	(and
	 (message (format "The directory %s does not contain an Org file." org-dir))
	 nil)))))

(get-hl-with-prop "~/Dev/Templates/Org" "build-tools")

(headline (:raw-value "Build Tools" :begin 4888 :end 29646 :pre-blank 0 :contents-begin 4902 :contents-end 29645 :level 1 :priority nil :tags nil :todo-keyword nil :todo-type nil :post-blank 1 :footnote-section-p nil :archivedp nil :commentedp nil :post-affiliated 4888 :FROM-FILE "Template" :CUSTOM_ID "build-tools" :APPENDIX "t" :title "Build Tools"))









;;; Add a keyword named 'bucket' just after the version macro.
;;; This function should be run from within the directory containing the Org file.
(defun add-bucket (org-file s3-bucket)
  "Add the name of the associated AWS S3 bucket to an Org templated file."
  (with-current-buffer (find-file-noselect org-file)
    (goto-char (point-min))
    (let* ((tree (org-element-parse-buffer))
	   ;; find the beginning position of the first headline to act as a limit
	   (hl1 (org-element-map tree (quote headline) (lambda (hl) (org-element-property :begin hl)) nil t)))
      ;; Check for the presence of a bucket keyword before the first headline
      (unless (re-search-forward "^#\\+bucket:" hl1 t)
	;; If no bucket keyword is found, search for a keyword MACRO with the value 'version'
	(org-element-map tree (quote keyword)
	  (lambda (kw) (when (and (string= "MACRO" (org-element-property :key kw))
				  (string-match-p "version" (org-element-property :value kw)))
			 ;; return the end position of the MACRO; subtract an empty line if there is one
			 (goto-char (- (org-element-property :end kw) (org-element-property :post-blank kw)))
			 (insert "#+bucket:" s3-bucket)
			 (newline)
			 (basic-save-buffer)
			 (message (format "Added bucket %s" s3-bucket))))
	  nil t)))))

(add-bucket "MasteringEmacs.org" "pinecone-forest.com")
nil

"Added bucket pinecone-forest.com"









(keyword (:key "MACRO" :value "version Version 0.0.108" :begin 148 :end 181 :post-blank 1 :post-affiliated 148 ...))
("TITLE" "SUBTITLE" "AUTHOR" "DATE" "MACRO" "TEXINFO" "TEXINFO" "CINDEX" "CINDEX" "CINDEX" "CINDEX" "CINDEX" ...)







((keyword (:key "MACRO" :value "version Version 0.0.107" :begin 148 :end 181 :post-blank 1 :post-affiliated 148 ...)))
#+end_comment

* List of Programs
:PROPERTIES:
:appendix: t
:END:
#+texinfo:@listoffloats Listing

* List of Examples
:PROPERTIES:
:appendix: t
:END:
#+texinfo:@listoffloats Example

* Copying
:PROPERTIES:
:copying:  t
:END:

Copyright \copy 2020 by {{{author}}}

* Concept Index
:PROPERTIES:
:index: cp
:appendix: yes
:END:

* Program Index
:PROPERTIES:
:index: pg
:appendix: yes
:END:

* Function Index
:PROPERTIES:
:index: fn
:appendix: yes
:END:

* Variable Index
:PROPERTIES:
:index: vr
:appendix: yes
:END:


* Configuration							   :noexport:
#+startup:content

#+todo: SOMEDAY(s@) TODO(t@) INPROGRESS(i@) WAIT(w@) | CANCEL(c@) DONE(d!)

#+options: H:4

#+texinfo_class: info
#+texinfo_header:
#+texinfo_post_header:
#+texinfo_dir_category:<DIR CATEGORY>
#+texinfo_dir_title:<DIR TITLE>
#+texinfo_dir_desc:<DIR DESCRIPTION>
#+texinfo_printed_title:SecureNodejsApps---A Guide to Securing Node.js Applications


* Footnotes

[fn:1]In the browser, add =index.text= to the end of the URL to see the source.

[fn:2]Markdown requires the standard Perl library module Digest::MD5.


* Local Variables						   :noexport:
# Local Variables:
# fill-column: 79
# indent-tabs-mode: t
# eval: (auto-fill-mode)
# time-stamp-pattern: "8/^\\#\\+date:%:y-%02m-%02d %02H:%02M$"
# End:
